# Compliant Kubernetes Deployment on AWS

This document describes how to set up Compliant Kubernetes on AWS.
The setup has two major parts:

1. Deploying at least two vanilla Kubernetes clusters
1. Deploying Compliant Kubernetes apps

Before starting, make sure you have [all necessary tools](getting-started.md).

## Setup

Choose names for your service cluster and workload clusters, as well as the DNS domain to expose the services inside the service cluster:

```bash
SERVICE_CLUSTER="testsc"
WORKLOAD_CLUSTERS=( "testwc0" )
BASE_DOMAIN="example.com"
```

!!! Note
    If you want to set up multiple workload clusters you can add more names.
    E.g. `WORKLOAD_CLUSTERS=( "testwc0" "testwc1" "testwc2" )`

    `SERVICE_CLUSTER` and each entry in `WORKLOAD_CLUSTERS` must be maximum 17 characters long.

## Deploying vanilla Kubernetes clusters

We suggest to set up Kubernetes clusters using kubespray.
If you haven't done so already, clone the Elastisys Compliant Kubernetes Kubespray repo as follows:

```bash
git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray
cd compliantkubernetes-kubespray
```

### Infrastructure Setup using Terraform

#### Expose AWS credentials to Terraform

We suggest exposing AWS credentials to Terraform via environment variables, so they are not accidentally left on the file-system:

```bash
export TF_VAR_AWS_ACCESS_KEY_ID="xyz" # Access key for AWS
export TF_VAR_AWS_SECRET_ACCESS_KEY="zyx" # Secret key for AWS
export TF_VAR_AWS_SSH_KEY_NAME="foo" # Name of the AWS key pair to use for the EC2 instances
export TF_VAR_AWS_DEFAULT_REGION="bar" # Region to use for all AWS resources
```

!!! tip
    We suggest generating the SSH key locally, then importing it to AWS.

#### Customize your infrastructure

Create a configuration for the service cluster and the workload cluster:

```bash
pushd kubespray
for CLUSTER in ${SERVICE_CLUSTER} "${WORKLOAD_CLUSTERS[@]}"; do
    cat contrib/terraform/aws/terraform.tfvars \
    | sed \
        -e "s@^aws_cluster_name =.*@aws_cluster_name = \"$CLUSTER\"@" \
        -e "s@^inventory_file =.*@inventory_file = \"../../../inventory/hosts-$CLUSTER\"@" \
        -e "s@^aws_kube_worker_size =.*@aws_kube_worker_size = \"t3.large\"@" \
    > inventory/terraform-$CLUSTER.tfvars
done
popd
```

Review and, if needed, adjust the files in `kubespray/inventory/`.

#### Initialize and Apply Terraform

```bash
pushd kubespray/contrib/terraform/aws
terraform init
for CLUSTER in ${SERVICE_CLUSTER} "${WORKLOAD_CLUSTERS[@]}"; do
    terraform apply \
        -var-file=../../../inventory/terraform-$CLUSTER.tfvars \
        -auto-approve \
        -state=../../../inventory/tfstate-$CLUSTER.tfstate
done
popd
```

!!! important
    The Terraform state is stored in `kubespray/inventory/tfstate-*`.
    It is precious.
    Consider backing it up or using [Terraform Cloud](https://www.terraform.io/docs/cloud/index.html).

#### Check that the Ansible inventory was properly generated

```bash
ls -l kubespray/inventory/hosts-*
```

You may also want to check the AWS Console if the infrastructure was created correctly:

![Kubespray AWS Console](../img/kubespray-aws-console.png)

### Deploying vanilla Kubernetes clusters using Kubespray

With the infrastructure provisioned, we can now deploy both the sc and wc Kubernetes clusters using kubespray.
Before trying any of the steps, make sure you are in the repo's root folder.

#### Init the Kubespray config in your config path

```bash
export CK8S_CONFIG_PATH=~/.ck8s/aws
export CK8S_PGP_FP=<your GPG key fingerprint>  # retrieve with gpg --list-secret-keys
```

```bash
for CLUSTER in ${SERVICE_CLUSTER} "${WORKLOAD_CLUSTERS[@]}"; do
    ./bin/ck8s-kubespray init $CLUSTER aws $CK8S_PGP_FP
done
```

!!! important
    The key in `~/.ssh/id_rsa` must be the private key of the key referenced in `TF_VAR_AWS_SSH_KEY_NAME` above.

#### Copy the inventories generated by Terraform above in the right place

```bash
for CLUSTER in ${SERVICE_CLUSTER} "${WORKLOAD_CLUSTERS[@]}"; do
    cp kubespray/inventory/hosts-$CLUSTER $CK8S_CONFIG_PATH/$CLUSTER-config/inventory.ini
done
```

#### Run kubespray to deploy the Kubernetes clusters

```bash
for CLUSTER in ${SERVICE_CLUSTER} "${WORKLOAD_CLUSTERS[@]}"; do
    ./bin/ck8s-kubespray apply $CLUSTER --flush-cache -e ansible_user=ubuntu
done
```

This may take up to 20 minutes per cluster.

#### Correct the Kubernetes API IP addresses

Find the DNS names of the load balancers fronting the API servers:

```bash
grep apiserver_loadbalancer $CK8S_CONFIG_PATH/*-config/inventory.ini
```

Locate the encrypted kubeconfigs `kube_config_*.yaml` and edit them using sops.
Copy the URL of the load balancer from inventory files shown above into `kube_config_*.yaml`.
Do not overwrite the port.

```bash
for CLUSTER in ${SERVICE_CLUSTER} "${WORKLOAD_CLUSTERS[@]}"; do
    sops $CK8S_CONFIG_PATH/.state/kube_config_$CLUSTER.yaml
done
```

#### Test access to the clusters as follows

```bash
for CLUSTER in ${SERVICE_CLUSTER} "${WORKLOAD_CLUSTERS[@]}"; do
    sops exec-file $CK8S_CONFIG_PATH/.state/kube_config_$CLUSTER.yaml \
        'kubectl --kubeconfig {} get nodes'
done
```

## Deploying Compliant Kubernetes Apps

{%
   include-markdown "common.md"
   start="<!--clone-apps-start-->"
   end="<!--clone-apps-stop-->"
   comments=false
%}

{%
   include-markdown "common.md"
   start="<!--init-apps-start-->"
   end="<!--init-apps-stop-->"
   comments=false
%}

{%
   include-markdown "common.md"
   start="<!--configure-apps-start-->"
   end="<!--configure-apps-stop-->"
   comments=false
%}

The following are the minimum change you should perform:

```yaml
# sc-config.yaml and wc-config.yaml
global:
  baseDomain: "set-me"  # set to $BASE_DOMAIN
  opsDomain: "set-me"  # set to ops.$BASE_DOMAIN
  issuer: letsencrypt-prod

objectStorage:
  type: "s3"
  s3:
    region: "set-me"  # Region for S3 buckets, e.g, eu-central-1
    regionEndpoint: "set-me"  # e.g., https://s3.us-west-1.amazonaws.com

fluentd:
  forwarder:
    useRegionEndpoint: "set-me"  # set it to either true or false

issuers:
  letsencrypt:
    email: "set-me"  # set this to an email to receive LetsEncrypt notifications
```

```yaml
# secrets.yaml
objectStorage:
  s3:
    accessKey: "set-me" #put your s3 accesskey
    secretKey: "set-me" #put your s3 secretKey
```

### Create placeholder DNS entries

To avoid negative caching and other surprises.
Create two placeholders as follows (feel free to use the "Import zone" feature of AWS Route53):

```bash
echo """
*.$BASE_DOMAIN     60s A 203.0.113.123
*.ops.$BASE_DOMAIN 60s A 203.0.113.123
"""
```

NOTE: 203.0.113.123 is in [TEST-NET-3](https://en.wikipedia.org/wiki/Reserved_IP_addresses) and okey to use as placeholder.

{%
   include-markdown "common.md"
   start="<!--install-apps-start-->"
   end="<!--install-apps-stop-->"
   comments=false
%}

{%
   include-markdown "common.md"
   start="<!--settling-start-->"
   end="<!--settling-stop-->"
   comments=false
%}

{%
   include-markdown "common.md"
   start="<!--dns-setup-start-->"
   end="<!--dns-setup-stop-->"
   comments=false
%}

### Setup required DNS entries

You will need to set up the following DNS entries.
First, determine the public IP of the load-balancer fronting the Ingress controller of the *service cluster*:

```bash
SC_INGRESS_LB_HOSTNAME=$(sops exec-file $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml 'kubectl --kubeconfig {} get -n ingress-nginx svc ingress-nginx-controller -o jsonpath={.status.loadBalancer.ingress[0].hostname}')
SC_INGRESS_LB_IP=$(dig +short $SC_INGRESS_LB_HOSTNAME | head -1)
echo $SC_INGRESS_LB_IP
```

Then, import the following zone in AWS Route53:

```bash
echo """
*.ops.$BASE_DOMAIN    60s A $SC_INGRESS_LB_IP
dex.$BASE_DOMAIN      60s A $SC_INGRESS_LB_IP
grafana.$BASE_DOMAIN  60s A $SC_INGRESS_LB_IP
harbor.$BASE_DOMAIN   60s A $SC_INGRESS_LB_IP
kibana.$BASE_DOMAIN   60s A $SC_INGRESS_LB_IP
"""
```

{%
   include-markdown "common.md"
   start="<!--testing-start-->"
   end="<!--testing-stop-->"
   comments=false
%}

## Teardown

{%
   include-markdown "common.md"
   start="<!--clean-apps-start-->"
   end="<!--clean-apps-stop-->"
   comments=false
%}

### Remove infrastructure

```bash
pushd kubespray/contrib/terraform/aws
for CLUSTER in ${SERVICE_CLUSTER} "${WORKLOAD_CLUSTERS[@]}"; do
    terraform destroy \
        -auto-approve \
        -state=../../../inventory/tfstate-$CLUSTER.tfstate
done
popd
```

## Further Reading

* [Compliant Kubernetes apps repo](https://github.com/elastisys/compliantkubernetes-apps)
* [Configurations option](https://github.com/elastisys/compliantkubernetes-apps#elastisys-compliant-kubernetes-apps)
